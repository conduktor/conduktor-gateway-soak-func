title: Merge clusters
services:
  kafka1:
    environment:
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: false
    properties:
      bootstrap.servers: localhost:29092,localhost:29093,localhost:29094
  kafka2:
    environment:
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: false
    properties:
      bootstrap.servers: localhost:29092,localhost:29093,localhost:29094
  kafka3:
    environment:
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: false
    properties:
      bootstrap.servers: localhost:29092,localhost:29093,localhost:29094
  kafka-europe1:
    docker:
      hostname: kafka-europe1
      container_name: kafka-europe1
      image: confluentinc/cp-kafka:latest
      ports:
        - "39092:39092"
      environment:
        KAFKA_BROKER_ID: 1
        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2801/europe
        KAFKA_LISTENERS: EXTERNAL_SAME_HOST://:39092,INTERNAL://:9092
        KAFKA_ADVERTISED_LISTENERS: EXTERNAL_SAME_HOST://localhost:39092,INTERNAL://kafka-europe1:9092
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL_SAME_HOST:PLAINTEXT
        KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
        KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
        KAFKA_LOG4J_LOGGERS: "kafka.authorizer.logger=INFO"
        KAFKA_LOG4J_ROOT_LOGLEVEL: WARN
        KAFKA_AUTO_CREATE_TOPICS_ENABLE: false
      depends_on:
        zookeeper:
          condition: service_healthy
      healthcheck:
        test: nc -zv kafka-europe1 9092 || exit 1
        interval: 5s
        retries: 25
    properties:
      bootstrap.servers: localhost:39092,localhost:39093,localhost:39094
  kafka-europe2:
    docker:
      hostname: kafka-europe2
      container_name: kafka-europe2
      image: confluentinc/cp-kafka:latest
      ports:
        - "39093:39093"
      environment:
        KAFKA_BROKER_ID: 2
        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2801/europe
        KAFKA_LISTENERS: EXTERNAL_SAME_HOST://:39093,INTERNAL://:9093
        KAFKA_ADVERTISED_LISTENERS: EXTERNAL_SAME_HOST://localhost:39093,INTERNAL://kafka-europe2:9093
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL_SAME_HOST:PLAINTEXT
        KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
        KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
        KAFKA_LOG4J_LOGGERS: "kafka.authorizer.logger=INFO"
        KAFKA_LOG4J_ROOT_LOGLEVEL: WARN
        KAFKA_AUTO_CREATE_TOPICS_ENABLE: false
      depends_on:
        zookeeper:
          condition: service_healthy
      healthcheck:
        test: nc -zv kafka-europe2 9093 || exit 1
        interval: 5s
        retries: 25
    properties:
      bootstrap.servers: localhost:39092,localhost:39093,localhost:39094
  kafka-europe3:
    docker:
      hostname: kafka-europe3
      container_name: kafka-europe3
      image: confluentinc/cp-kafka:latest
      ports:
        - "39094:39094"
      environment:
        KAFKA_BROKER_ID: 3
        KAFKA_ZOOKEEPER_CONNECT: zookeeper:2801/europe
        KAFKA_LISTENERS: EXTERNAL_SAME_HOST://:39094,INTERNAL://:9094
        KAFKA_ADVERTISED_LISTENERS: EXTERNAL_SAME_HOST://localhost:39094,INTERNAL://kafka-europe3:9094
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL_SAME_HOST:PLAINTEXT
        KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
        KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
        KAFKA_LOG4J_LOGGERS: "kafka.authorizer.logger=INFO"
        KAFKA_LOG4J_ROOT_LOGLEVEL: WARN
        KAFKA_AUTO_CREATE_TOPICS_ENABLE: false
      depends_on:
        zookeeper:
          condition: service_healthy
      healthcheck:
        test: nc -zv kafka-europe3 9094 || exit 1
        interval: 5s
        retries: 25
    properties:
      bootstrap.servers: localhost:39092,localhost:39093,localhost:39094
  gateway1:
    docker:
      environment:
        GATEWAY_ADVERTISED_HOST: localhost
        GATEWAY_FEATURE_FLAGS_MULTI_TENANCY: true
        GATEWAY_SECURITY_PROTOCOL: PLAINTEXT
        GATEWAY_CLUSTER_ID: private
        GATEWAY_PORT_COUNT: 10
        GATEWAY_BACKEND_KAFKA_SELECTOR: 'file : { path:  /config/clusters.yaml}'
      volumes:
        - type: bind
          source: .
          target: /config
          read_only: true
    properties:
      bootstrap.servers: localhost:6969
      gateway.host: http://localhost:8888

  gateway2:
    docker:
      environment:
        GATEWAY_ADVERTISED_HOST: localhost
        GATEWAY_FEATURE_FLAGS_MULTI_TENANCY: true
        GATEWAY_SECURITY_PROTOCOL: PLAINTEXT
        GATEWAY_CLUSTER_ID: private
        GATEWAY_PORT_COUNT: 10
        GATEWAY_BACKEND_KAFKA_SELECTOR: 'file : { path:  /config/clusters.yaml}'
      volumes:
        - type: bind
          source: .
          target: /config
          read_only: true
    properties:
      bootstrap.servers: localhost:6969
      gateway.host: http://localhost:8889

actions:

  - type: INTRODUCTION
    title: What is merge clusters
    markdown: |
      Imagine being able to bring all your Kafka clusters together into a instance for clients to access.  
      
      Conduktor's Merge Clusters feature does just this.
      
      Conduktor Gateway can sit in front of multiple Kafka clusters, making them appear as a single cluster to end users.
      
      Test applications can read live production data from a production cluster, for realistic testing, without affecting or altering that production data, and then use test topics on a test cluster to write data, all without having to reconfigure any applications.  
      There is no need to do expensive replication of data to get useful and accurate testing data.
      
      Failover is made much simpler by Conduktor Gateway managing the routing in your Kafka environment.  
      Say you want to do a managed failover from cluster A to cluster B as part of planned maintenance.  
      If your applications connects to Conduktor Gateway, to do this move just alter the Gateway configuration, and your applications are routed to the new cluster, with no need for application restarts or changes.
      
      Data costs and complexity are reduced by consolidating data from multiple sources in to a central resource.  
      For example, you might have data centres in multiple regions, perhaps because they are close to the applications using that data, but also because of legal requirements to keep data stored in a particular region.
      
      Conduktor Gateway gives a central consuming application access to data in all these regions, with a single set of connection configuration and a single set of credentials. Requests are routed to the right backend cluster, avoiding replication and avoiding complicated sets of configuration. 
      Gateway manages the complexity of the multiple backends, allowing the application team to focus on getting the most value from the easily accessible data.
      
      
      This example demonstrate this last use case - how to bring together data from multiple sources, into a single access point that can be read by a single consumer.


  - type: ASCIINEMA

  - type: FILE
    filename: docker-compose.yaml

  - type: FILE
    title: Review the Gateway configuration
    filename: clusters.yaml
    markdown: |
      The Kafka clusters used by Gateway are stored in `clusters.yaml` and this is mounted into the Gateway container.

  - type: DOCKER
    command: docker compose up --detach --wait

  - type: CREATE_TOPICS
    kafka: kafka1
    topics:
      - name: cars
        replicationFactor: 1
        partitions: 1

  - type: PRODUCE
    kafka: kafka1
    topic: cars
    messages:
      - value: '{"car: "Chevrolet", "region": "us"}'

  - type: CONSUME
    kafka: kafka1
    topic: cars
    maxMessages: 1
    assertSize: 1

  - type: CREATE_TOPICS
    kafka: kafka-europe1
    topics:
      - name: cars
        replicationFactor: 1
        partitions: 1

  - type: PRODUCE
    kafka: kafka-europe1
    topic: cars
    messages:
      - value: '{"car: "Ferrari", "region": "europe"}'

  - type: CONSUME
    kafka: kafka-europe1
    topic: cars
    maxMessages: 1
    assertSize: 1

  - type: LIST_TOPICS
    kafka: gateway1

  - type: ADD_TOPIC_MAPPING
    gateway: gateway1
    vcluster: passthrough
    logicalTopicName: "cars_us"
    mapping: {
      "clusterId": "main",
      "physicalTopicName": "cars"
    }

  - type: ADD_TOPIC_MAPPING
    gateway: gateway1
    vcluster: passthrough
    logicalTopicName: "cars_europe"
    mapping: {
      "clusterId": "europe",
      "physicalTopicName": "cars"
    }

  - type: LIST_TOPICS
    kafka: gateway1
    assertExists:
      - cars_us
      - cars_europe

  - type: CONSUME
    kafka: gateway1
    topic: cars_us
    maxMessages: 1
    assertSize: 1
    assertions:
      - description: Confirm US
        value:
          operator: containsIgnoreCase
          expected: 'us'

  - type: CONSUME
    kafka: gateway1
    topic: cars_europe
    maxMessages: 1
    assertSize: 1
    assertions:
      - description: Confirm Europe
        value:
          operator: containsIgnoreCase
          expected: 'europe'

  - type: DOCKER
    command: docker compose down --volumes

  - type: CONCLUSION
    markdown: |
      With merge cluster seamlessly pick and choose topics from multiple different kafka clusters!

